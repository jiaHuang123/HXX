%
% ---------------------------------------------------------------
% Copyright (C) 2012-2018 Gang Li
% ---------------------------------------------------------------
%
% This work is the default powerdot-tuliplab style test file and may be
% distributed and/or modified under the conditions of the LaTeX Project Public
% License, either version 1.3 of this license or (at your option) any later
% version. The latest version of this license is in
% http://www.latex-project.org/lppl.txt and version 1.3 or later is part of all
% distributions of LaTeX version 2003/12/01 or later.
%
% This work has the LPPL maintenance status "maintained".
%
% This Current Maintainer of this work is Gang Li.
%
%

\documentclass[
 size=14pt,
 paper=smartboard,  %a4paper, smartboard, screen
 mode=present, 		%present, handout, print
 display=slides, 	% slidesnotes, notes, slides
 style=tuliplab,  	% TULIP Lab style
 pauseslide,
 fleqn,leqno]{powerdot}


\usepackage{cancel}
\usepackage{caption}
\usepackage{stackengine}
\usepackage{smartdiagram}
\usepackage{attrib}
\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{amsthm} 
\usepackage{mathtools}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage{boxedminipage}
\usepackage{rotate}
\usepackage{calc}
\usepackage[absolute]{textpos}
\usepackage{psfrag,overpic}
\usepackage{fouriernc}
\usepackage{pstricks,pst-3d,pst-grad,pstricks-add,pst-text,pst-node,pst-tree}
\usepackage{moreverb,epsfig,subfigure}
\usepackage{color}
\usepackage{booktabs}
\usepackage{etex}
\usepackage{breqn}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{bibentry}
\usepackage{gitinfo2}
\usepackage{siunitx}
\usepackage{nicefrac}
%\usepackage{geometry}
%\geometry{verbose,letterpaper}
\usepackage{media9}
\usepackage{animate}
%\usepackage{movie15}
\usepackage{auto-pst-pdf}

\usepackage{breakurl}
\usepackage{fontawesome}
\usepackage{xcolor}
\usepackage{multicol}



\usepackage{verbatim}
\usepackage[utf8]{inputenc}
\usepackage{dtk-logos}
\usepackage{tikz}
\usepackage{adigraph}
%\usepackage{tkz-graph}
\usepackage{hyperref}
%\usepackage{ulem}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{fontawesome}


\usepackage{todonotes}
% \usepackage{pst-rel-points}
\usepackage{animate}
\usepackage{fontawesome}

\usepackage{listings}
\lstset{frameround=fttt,
frame=trBL,
stringstyle=\ttfamily,
backgroundcolor=\color{yellow!20},
basicstyle=\footnotesize\ttfamily}
\lstnewenvironment{code}{
\lstset{frame=single,escapeinside=`',
backgroundcolor=\color{yellow!20},
basicstyle=\footnotesize\ttfamily}
}{}


\usepackage{hyperref}
\hypersetup{ % TODO: PDF meta Data
  pdftitle={Presentation Title},
  pdfauthor={Gang Li},
  pdfpagemode={FullScreen},
  pdfborder={0 0 0}
}


% \usepackage{auto-pst-pdf}
% package to show source code

\definecolor{LightGray}{rgb}{0.9,0.9,0.9}
\newlength{\pixel}\setlength\pixel{0.000714285714\slidewidth}
\setlength{\TPHorizModule}{\slidewidth}
\setlength{\TPVertModule}{\slideheight}
\newcommand\highlight[1]{\fbox{#1}}
\newcommand\icite[1]{{\footnotesize [#1]}}

\newcommand\twotonebox[2]{\fcolorbox{pdcolor2}{pdcolor2}
{#1\vphantom{#2}}\fcolorbox{pdcolor2}{white}{#2\vphantom{#1}}}
\newcommand\twotoneboxo[2]{\fcolorbox{pdcolor2}{pdcolor2}
{#1}\fcolorbox{pdcolor2}{white}{#2}}
\newcommand\vpspace[1]{\vphantom{\vspace{#1}}}
\newcommand\hpspace[1]{\hphantom{\hspace{#1}}}
\newcommand\COMMENT[1]{}

\newcommand\placepos[3]{\hbox to\z@{\kern#1
        \raisebox{-#2}[\z@][\z@]{#3}\hss}\ignorespaces}

\renewcommand{\baselinestretch}{1.2}


\newcommand{\draftnote}[3]{
	\todo[author=#2,color=#1!30,size=\footnotesize]{\textsf{#3}}	}
% TODO: add yourself here:
%
\newcommand{\gangli}[1]{\draftnote{blue}{GLi:}{#1}}
\newcommand{\shaoni}[1]{\draftnote{green}{sn:}{#1}}
\newcommand{\gliMarker}
	{\todo[author=GLi,size=\tiny,inline,color=blue!40]
	{Gang Li has worked up to here.}}
\newcommand{\snMarker}
	{\todo[author=Sn,size=\tiny,inline,color=green!40]
	{Shaoni has worked up to here.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title
% TODO: Customize to your Own Title, Name, Address
%
\title{Tweet Sentiment Extraction}
\author{
Jia Huang
\\
\\Xi'an Shiyou University
%\\Deakin University
\\Chinese Academy of Sciences
%\\ \today
}
%\date{\gitCommitterDate}


% Customize the setting of slides
\pdsetup{
% TODO: Customize the left footer, and right footer
rf=\href{http://www.tulip.org.au}{
Last Changed by: \textsc{\gitCommitterName}\ \gitVtagn-\gitAbbrevHash\ (\gitAuthorDate)
},
cf={Tweet Sentiment Extraction},
}


\begin{document}

\maketitle

%\begin{slide}{Overview}
%\tableofcontents[content=sections]
%\end{slide}


%%==========================================================================================
%%
\begin{slide}[toc=,bm=]{Overview}
\tableofcontents[content=currentsection,type=1]
\end{slide}
%%
%%==========================================================================================


\section{Project Overview}


%%==========================================================================================
%%
\begin{slide}{Project Background And Purpose}
\begin{center}
\twotonebox{\rotatebox{95}{Defn}}{\parbox{.89\textwidth}
{
  \begin{itemize}
    \item Background
    \\With all of the tweets circulating every second it is hard 
    to tell whether the sentiment behind a specific tweet will impact 
    a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is 
    important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description?
    \item Purpose
    \\In this competition we've extracted support phrases from Figure Eight's Data
     for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets
      with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this 
      competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what
       word or phrase best supports it.
    \end{itemize}
}}

\end{center}
\bigskip
\begin{center}
\begin{tabular}{c| c c c c }
\toprule
\midrule

\bottomrule
\end{tabular}
\end{center}
\bigskip

%%==========================================================================================
\begin{note}
First, I will introduce the problem definition.
In the real life,
a teacher may be interested in the characteristics that
make one student obvious different from others.
Or,
NBA sports coaches would prefer to
know the advantages and disadvantages of one player.
Here, the player can be regarded as a query object.

For example, team A has five players,
each player has four features.
The NBA sports coaches may want to know the features of
player $1$ that are different from others.

The above example can be seen as outlying aspects mining.
The main purpose of outlying aspects mining is to identify
the outstanding features of the query object.
\end{note}
%%==========================================================================================

\end{slide}
%%
%%==========================================================================================


%%
%%==========================================================================================


\section{Data Pre-Processing}


%%==========================================================================================
%%
\begin{slide}{Date Processing}
%Related Work - Outlying Aspects Mining
Two data sets were given during the match: train.csv 
and test.csv.Train.csv data were used to construct the model and 
to predict test.csv data.
Now let's take a look at the training data provided by this 
competition, do some exploratory data analysis work, and learn more about
 the content of this training set.~\\
 The data sets train.csv and test.csv are respectively 27408 and 3534 pieces 
 of data.The data is described in terms of four attributes.

\bigskip
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{kaggle/01.1.eps}
  \caption{}
\end{figure}




%%==========================================================================================
\begin{note}
Let me introduce two existing methods:
Feature selection and score-and-search.

For feature selection,
the query point can be regarded as positive class and
the rest of the data can be regarded as negative class,
selected the features that best distinguish the two classes.

The advantages of this method are easy to operate,
and it's able to resolve dimensionality bias.
However, it has some drawbacks.
Firstly,
positive and negative classes are Not balanced,
secondly,
it can't quantify the outlying degree correctly.
Most importantly,
it doesn't identify group outlying aspects.
\end{note}
%%==========================================================================================

\end{slide}
%%
%%==========================================================================================

%%
%%==========================================================================================
\begin{slide}{Feature Item}
  As you can see from the table above, the four feature
  items in the training set are textID, Text, and selected_text sentiment.
  ~\\
  ~\\
  \bigskip
  \begin{itemize}
    \item textID
    \begin{itemize}
      \item Write the ID of the comment.
    \end{itemize}
    \item Text
    \begin{itemize}
      \item The specific content of the comment.
    \end{itemize}
    \item Selected_text
    \begin{itemize}
      \item Are the keywords that we have chosen to 
      judge the emotional state of the comment.
    \end{itemize}
    \item Sentiment
    \begin{itemize}
      \item The emotional polarity of the sentence.
    \end{itemize}
  \end{itemize}
\end{slide}

%%
%%==========================================================================================
\begin{slide}{Features Item}
  \begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{kaggle/01.3.eps}
  \end{figure}
  
  \bigskip
  There are three types of sentence emotional polarity.Neurtal
   indicates that the sentence is emotionally neutral, positive
    means the comment is positive, and negative means the 
    comment is negative.
\end{slide}
%%
%%==================================================================





%%==========================================================================================

\section{Constructing Dataset Generator}

%%==========================================================================================
%%
\begin{slide}{Similarity}
  Look at the Jaccard similarity between Text and selected_text.
  \bigskip
  \begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{kaggle/1.8.eps}
  \end{figure}
\end{slide}
%%
%%===========================================================================================



%%
%%=============================================================================================
\begin{slide}[toc=,bm=]{The kernel distribution graph of word length}

\begin{figure}[htbp]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{kaggle/1.1.eps}
    \vspace{0.4em}
    \caption{ }
  \end{minipage}
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{kaggle/1.2.eps}
    \vspace{0.4em}
    \caption{}
  \end{minipage}
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{kaggle/1.3.eps}
    \vspace{0.4em}
    \caption{}
  \end{minipage}
\end{figure}

%%==========================================================================================
\begin{note}
In conclusion,
we firstly formalized the problem of
group outlying aspects mining,

Then proposed a novel method GOAM algorithm to address the problem of
group outlying aspects mining,
and the proposed method use pruning to reduce time complexity
while identifying the suitable set of outlying features for the interested group.

Thank you and any question?
\end{note}
%%==========================================================================================

\end{slide}
%%
%%==========================================================================================

\begin{slide}{The kernel distribution graph of word length}

  \centering
  ~\
  As can be seen from the above figure, the Jaccard similarity of 
  positive or negative text and selected_text has two sharp kurtosis 
  around 1.0 or 0.1.The word length difference also has two kurtosis,
   where the difference of 0 is a sharp kurtosis.That means that a large 
   percentage of positive or negative text is the same as selected_text.
  

\end{slide}
%%
%%===========================================================================================



%%
%%========================================================================


%%
%%%%===============================================================

%%
%%===================================================================



\begin{slide}{The word cloud}
  
  \begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=0.8\textwidth]{kaggle/1.4.eps}
      \vspace{0.4em}
      \caption{}
    \end{minipage}
    \begin{minipage}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=0.8\textwidth]{kaggle/1.5.eps}
      \vspace{0.4em}
      \caption{}
    \end{minipage}
    \begin{minipage}[t]{0.48\textwidth}
      \centering
      \includegraphics[width=0.8\textwidth]{kaggle/1.6.eps}
      \vspace{0.4em}
      \caption{}
    \end{minipage}
  \end{figure}
\end{slide}

%%
%%=========================================================================

%%
%%==========================================================================

\section{Model Construction and Training} 


%%
%%==============================================================================

\begin{slide}{}
  \begin{itemize}
    \item Using Facebook's Roberta model, we first need to build Tokenizer to convert the training text into Roberta's token.
    \begin{itemize}
      \item Build Tokenizer to convert the text of the training set and test set to Token.
    \end{itemize}
  \end{itemize}
  

  \bigskip
  To build the Roberta model,first load the pre-trained model,the input of the model 
is the above converted text token,the output is the vecor of BatchxMAX_LENx768, 
  add two Q\&A heads,one of which is responsible for predicting 
   the beginning position of the answer and the other is responsible 
   for predicting the end position of the answer.By making 
   a full connection layer of 768x1 to the output vector to get the Head,
   the output becomes the vector of BatchxMAX_LENx1,then rehaspe is the 
   vector of BatchxMAX_LEN,and then Softmax.
 
\end{slide}

%%===========================================================================================
%%
\begin{slide}{}
  \begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{kaggle/1.9.eps}
  \end{figure}
\end{slide}
%%
%%========================================================================================


%%
%%==================================================================================




%%
%%==============================================================================




%%==========================================================================================
%%



% TODO: Contact Page

\end{document}

\endinput
